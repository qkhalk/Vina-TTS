{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VieNeu-TTS Google Colab Backend\n",
    "\n",
    "This notebook sets up a remote TTS backend for VieNeu-TTS using Google Colab's free GPU.\n",
    "\n",
    "**‚ö†Ô∏è Keep this notebook running** - Closing it will disconnect your TTS server.\n",
    "\n",
    "**Runtime**: GPU (T4) recommended | CPU also supported\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Mount Google Drive (for model caching)\n",
    "\n",
    "Uncomment and run this cell to cache models on Google Drive (saves re-download time on restarts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import os\n",
    "# os.environ['HF_HOME'] = '/content/drive/MyDrive/huggingface_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install System Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq\n",
    "!apt-get install -y espeak-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install uv Package Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (modern Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add uv to PATH for current session\n",
    "import os\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Verify installation\n",
    "!uv --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone and Install VieNeu-TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/qkhalk/Vina-TTS.git\n",
    "%cd Vina-TTS\n",
    "\n",
    "# Use uv to install all dependencies (more stable than pip)\n",
    "!uv sync\n",
    "\n",
    "# Install additional packages needed for Colab server\n",
    "!uv pip install fastapi uvicorn pyngrok nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" uv pip install llama-cpp-python --force-reinstall --no-cache-dir\n",
    "!uv pip install lmdeploy\n",
    "!uv pip install triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure and Start FastAPI Server\n",
    "\n",
    "This cell:\n",
    "- Activates the uv virtual environment\n",
    "- Creates a FastAPI server for TTS synthesis\n",
    "- Loads the model ({{ backbone_repo }}, {{ codec_repo }})\n",
    "- Starts ngrok tunnel for remote access\n",
    "- Displays connection URL and auth token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate uv virtual environment\n",
    "import sys\n",
    "sys.path.insert(0, '/content/Vina-TTS/.venv/lib/python3.12/site-packages')\n",
    "\n",
    "import os\n",
    "import secrets\n",
    "import base64\n",
    "from fastapi import FastAPI, HTTPException, Depends, status\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import torch\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Authentication token (copy this to your admin UI)\n",
    "AUTH_TOKEN = \"{{ auth_token }}\"\n",
    "\n",
    "# Model configuration\n",
    "BACKBONE_REPO = \"{{ backbone_repo }}\"\n",
    "CODEC_REPO = \"{{ codec_repo }}\"\n",
    "DEVICE = \"{{ device }}\"\n",
    "ENABLE_TRITON = {{ enable_triton }}\n",
    "MAX_BATCH_SIZE = {{ max_batch_size }}\n",
    "\n",
    "# Convert 'auto' to actual device\n",
    "if DEVICE.lower() == \"auto\":\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Auto-detected device: {DEVICE}\")\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(title=\"VieNeu-TTS Colab Backend\")\n",
    "security = HTTPBearer()\n",
    "\n",
    "# Global TTS model\n",
    "tts_model = None\n",
    "\n",
    "def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    if credentials.credentials != AUTH_TOKEN:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Invalid authentication token\"\n",
    "        )\n",
    "    return credentials.credentials\n",
    "\n",
    "class TTSRequest(BaseModel):\n",
    "    text: str\n",
    "    voice_sample_path: str\n",
    "    voice_transcript: str\n",
    "    speed: float = 1.0\n",
    "    watermark: bool = True\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global tts_model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ Loading VieNeu-TTS Model...\")\n",
    "    print(f\"   Backbone: {BACKBONE_REPO}\")\n",
    "    print(f\"   Codec: {CODEC_REPO}\")\n",
    "    print(f\"   Device: {DEVICE}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    from vieneu_tts import VieNeuTTS, FastVieNeuTTS\n",
    "    \n",
    "    \n",
    "    # Use FastVieNeuTTS for GPU (non-GGUF) for better performance\n",
    "    use_fast = DEVICE == \"cuda\" and \"gguf\" not in BACKBONE_REPO.lower()\n",
    "    \n",
    "    if use_fast:\n",
    "        tts_model = FastVieNeuTTS(\n",
    "            backbone_repo=BACKBONE_REPO,\n",
    "            backbone_device=\"cuda\",\n",
    "            codec_repo=CODEC_REPO,\n",
    "            codec_device=\"cuda\",\n",
    "            memory_util=0.3,\n",
    "            tp=1,\n",
    "            enable_prefix_caching=True,\n",
    "            enable_triton=ENABLE_TRITON,\n",
    "            max_batch_size=MAX_BATCH_SIZE\n",
    "        )\n",
    "    else:\n",
    "        tts_model = VieNeuTTS(\n",
    "            backbone_repo=BACKBONE_REPO,\n",
    "            backbone_device=DEVICE,\n",
    "            codec_repo=CODEC_REPO,\n",
    "            codec_device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "    print(\"‚úÖ Model loaded successfully!\\n\")\n",
    "\n",
    "@app.post(\"/tts/synthesize\")\n",
    "async def synthesize(request: TTSRequest, token: str = Depends(verify_token)):\n",
    "    if tts_model is None:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Model not loaded\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        audio_array, sample_rate = tts_model.tts(\n",
    "            text=request.text,\n",
    "            voice_sample_path=request.voice_sample_path,\n",
    "            voice_transcript=request.voice_transcript,\n",
    "            speed=request.speed,\n",
    "            watermark=request.watermark\n",
    "        )\n",
    "        \n",
    "        import soundfile as sf\n",
    "        import io\n",
    "        buffer = io.BytesIO()\n",
    "        sf.write(buffer, audio_array, sample_rate, format='WAV')\n",
    "        audio_bytes = buffer.getvalue()\n",
    "        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "        \n",
    "        return {\n",
    "            \"audio_base64\": audio_base64,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"duration_ms\": int(len(audio_array) / sample_rate * 1000)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_detail = f\"TTS synthesis failed: {str(e)}\"\n",
    "        print(f\"\\n‚ùå ERROR in /tts/synthesize:\")\n",
    "        print(error_detail)\n",
    "        print(\"\\nFull traceback:\")\n",
    "        traceback.print_exc()\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=error_detail\n",
    "        )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check(token: str = Depends(verify_token)):\n",
    "    gpu_memory_used = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"model_loaded\": tts_model is not None,\n",
    "        \"gpu_memory_used_gb\": gpu_memory_used,\n",
    "        \"gpu_available\": torch.cuda.is_available()\n",
    "    }\n",
    "\n",
    "# Start ngrok tunnel\n",
    "# IMPORTANT: Get your ngrok token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "NGROK_TOKEN = \"YOUR_NGROK_TOKEN\"  # Replace with your actual ngrok token\n",
    "\n",
    "if NGROK_TOKEN == \"YOUR_NGROK_TOKEN\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: You need to add your ngrok token!\")\n",
    "    print(\"üìç Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "    print(\"üìù Replace 'YOUR_NGROK_TOKEN' with your actual token in the cell above\")\n",
    "    raise ValueError(\"ngrok token not configured\")\n",
    "\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ VieNeu-TTS Colab Backend is READY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìç Endpoint URL: {public_url}\")\n",
    "print(f\"üîë Auth Token: {AUTH_TOKEN}\")\n",
    "print(\"\\nüìã Copy the above URL and Token to your Admin UI\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this cell running - Don't stop execution!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Start server (Colab-compatible)\n",
    "import asyncio\n",
    "from uvicorn import Config, Server\n",
    "\n",
    "config = Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "server = Server(config)\n",
    "\n",
    "# Run server in the existing event loop\n",
    "await server.serve()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
