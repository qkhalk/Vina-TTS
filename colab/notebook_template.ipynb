{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VieNeu-TTS Google Colab Backend\n",
    "\n",
    "This notebook sets up a remote TTS backend for VieNeu-TTS using Google Colab's free GPU.\n",
    "\n",
    "**‚ö†Ô∏è Keep this notebook running** - Closing it will disconnect your TTS server.\n",
    "\n",
    "**Runtime**: GPU (T4) recommended | CPU also supported\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Mount Google Drive (for model caching)\n",
    "\n",
    "Uncomment and run this cell to cache models on Google Drive (saves re-download time on restarts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import os\n",
    "# os.environ['HF_HOME'] = '/content/drive/MyDrive/huggingface_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install System Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq\n",
    "!apt-get install -y espeak-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q fastapi uvicorn pyngrok requests pydantic\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers accelerate phonemizer librosa soundfile pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone VieNeu-TTS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pnnbao-ump/Vina-TTS.git\n",
    "%cd Vina-TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure and Start FastAPI Server\n",
    "\n",
    "This cell:\n",
    "- Creates a FastAPI server for TTS synthesis\n",
    "- Loads the model ({{ backbone_repo }}, {{ codec_repo }})\n",
    "- Starts ngrok tunnel for remote access\n",
    "- Displays connection URL and auth token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import secrets\n",
    "import base64\n",
    "from fastapi import FastAPI, HTTPException, Depends, status\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import torch\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Authentication token (copy this to your admin UI)\n",
    "AUTH_TOKEN = \"{{ auth_token }}\"\n",
    "\n",
    "# Model configuration\n",
    "BACKBONE_REPO = \"{{ backbone_repo }}\"\n",
    "CODEC_REPO = \"{{ codec_repo }}\"\n",
    "DEVICE = \"{{ device }}\"\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(title=\"VieNeu-TTS Colab Backend\")\n",
    "security = HTTPBearer()\n",
    "\n",
    "# Global TTS model\n",
    "tts_model = None\n",
    "\n",
    "def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    if credentials.credentials != AUTH_TOKEN:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Invalid authentication token\"\n",
    "        )\n",
    "    return credentials.credentials\n",
    "\n",
    "class TTSRequest(BaseModel):\n",
    "    text: str\n",
    "    voice_sample_path: str\n",
    "    voice_transcript: str\n",
    "    speed: float = 1.0\n",
    "    watermark: bool = True\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global tts_model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ Loading VieNeu-TTS Model...\")\n",
    "    print(f\"   Backbone: {BACKBONE_REPO}\")\n",
    "    print(f\"   Codec: {CODEC_REPO}\")\n",
    "    print(f\"   Device: {DEVICE}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    from vieneu_tts import VieNeuTTS\n",
    "    \n",
    "    tts_model = VieNeuTTS(\n",
    "        backbone_repo=BACKBONE_REPO,\n",
    "        backbone_device=DEVICE,\n",
    "        codec_repo=CODEC_REPO,\n",
    "        codec_device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully!\\n\")\n",
    "\n",
    "@app.post(\"/tts/synthesize\")\n",
    "async def synthesize(request: TTSRequest, token: str = Depends(verify_token)):\n",
    "    if tts_model is None:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Model not loaded\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        audio_array, sample_rate = tts_model.tts(\n",
    "            text=request.text,\n",
    "            voice_sample_path=request.voice_sample_path,\n",
    "            voice_transcript=request.voice_transcript,\n",
    "            speed=request.speed,\n",
    "            watermark=request.watermark\n",
    "        )\n",
    "        \n",
    "        import soundfile as sf\n",
    "        import io\n",
    "        buffer = io.BytesIO()\n",
    "        sf.write(buffer, audio_array, sample_rate, format='WAV')\n",
    "        audio_bytes = buffer.getvalue()\n",
    "        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "        \n",
    "        return {\n",
    "            \"audio_base64\": audio_base64,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"duration_ms\": int(len(audio_array) / sample_rate * 1000)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"TTS synthesis failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check(token: str = Depends(verify_token)):\n",
    "    gpu_memory_used = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"model_loaded\": tts_model is not None,\n",
    "        \"gpu_memory_used_gb\": gpu_memory_used,\n",
    "        \"gpu_available\": torch.cuda.is_available()\n",
    "    }\n",
    "\n",
    "# Start ngrok tunnel\n",
    "ngrok.set_auth_token(\"YOUR_NGROK_TOKEN\")  # Optional: add your ngrok token for custom domains\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ VieNeu-TTS Colab Backend is READY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìç Endpoint URL: {public_url}\")\n",
    "print(f\"üîë Auth Token: {AUTH_TOKEN}\")\n",
    "print(\"\\nüìã Copy the above URL and Token to your Admin UI\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this cell running - Don't stop execution!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Start server\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
